{
	"name": "PredictNotebook",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "APSparkPoolST",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0a8955f1-ca08-495e-8e09-1284d3bd7084"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/d633e14e-b627-40ae-8c9a-7f0dc15c2bd7/resourceGroups/rauljareno/providers/Microsoft.Synapse/workspaces/demosynapsest/bigDataPools/APSparkPoolST",
				"name": "APSparkPoolST",
				"type": "Spark",
				"endpoint": "https://demosynapsest.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/APSparkPoolST",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#Import libraries\r\n",
					"from pyspark.sql.functions import col, pandas_udf,udf,lit\r\n",
					"from azureml.core import Workspace\r\n",
					"from azureml.core.authentication import ServicePrincipalAuthentication\r\n",
					"import azure.synapse.ml.predict as pcontext\r\n",
					"import azure.synapse.ml.predict.utils._logger as synapse_predict_logger"
				],
				"execution_count": 75
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Set input data path\r\n",
					"DATA_FILE = \"abfss://raw@adldemost.dfs.core.windows.net/raw/MLExample/credit_test.csv\"\r\n",
					"\r\n",
					"#Set AML URI, if trained model is registered in AML\r\n",
					"AML_MODEL_URI = \"aml://demosynapsest-loans_training-20220310041342-Best:1\" #In URI \":x\" signifies model version in AML. You can   choose which model version you want to run. If \":x\" is not provided then by default   latest version will be picked.\r\n",
					"\r\n",
					"#Define model return type\r\n",
					"RETURN_TYPES = \"float\" # for ex: int, float etc. PySpark data types are supported\r\n",
					"\r\n",
					"#Define model runtime. This supports only mlflow\r\n",
					"RUNTIME = \"mlflow\""
				],
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#AML workspace authentication using linked service\r\n",
					"from notebookutils.mssparkutils import azureML\r\n",
					"ws = azureML.getWorkspace(\"azureMLst\") #   \"<linked_service_name>\" is the linked service name, not AML workspace name. Also, linked   service supports MSI and service principal both"
				],
				"execution_count": 77
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Enable SynapseML predict\r\n",
					"spark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")"
				],
				"execution_count": 78
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Bind model within Spark session\r\n",
					"model = pcontext.bind_model(\r\n",
					"    return_types=RETURN_TYPES, \r\n",
					"    runtime=RUNTIME, \r\n",
					"    model_alias=\"LoansModel\", #This alias will be used in PREDICT call to refer  this   model\r\n",
					"    model_uri=\"aml://demosynapsest-loans_training-20220310041342-Best:1\", #In case of AML, it will be AML_MODEL_URI\r\n",
					"    aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\r\n",
					"    ).register()"
				],
				"execution_count": 79
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import os\r\n",
					"os.getcwd()"
				],
				"execution_count": 80
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Read data from ADLS\r\n",
					"DATA_FILE=\"abfss://synapse@adldemost.dfs.core.windows.net/raw/MLExample/credit_test.csv\"\r\n",
					"df = spark.read.format(\"csv\").option(\"header\", \"true\").csv(DATA_FILE, inferSchema=True)\r\n",
					"df.createOrReplaceTempView('TempLoansView')"
				],
				"execution_count": 95
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = df.drop('LoanID')"
				],
				"execution_count": 96
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.metadata.get_input_schema()"
				],
				"execution_count": 99
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Call PREDICT using Spark SQL API\r\n",
					"\r\n",
					"predictions = spark.sql(\r\n",
					"                \"\"\"\r\n",
					"                    SELECT PREDICT('LoansModel',\r\n",
					"                            *) AS predict \r\n",
					"                    FROM TempLoansView\r\n",
					"                \"\"\"\r\n",
					"            ).show()"
				],
				"execution_count": 97
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Call PREDICT using user defined function (UDF)\r\n",
					"df.withColumn(\"PREDICT\",model.udf(lit(\"LoansModel\"),*df.columns)).show()"
				],
				"execution_count": 98
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Call PREDICT using Transformer API\r\n",
					"\r\n",
					"columns = ['tipped', 'paymentType', 'passengerCount', 'tripDistance','tripTimeSecs','pickupTimeBin'] # for ex. df[\"empid\",\"empname\"]\r\n",
					"\r\n",
					"tranformer = model.create_transformer().setInputCols(columns).setOutputCol(\"PREDICT\")\r\n",
					"\r\n",
					"tranformer.transform(df).show()"
				],
				"execution_count": 24
			}
		]
	}
}